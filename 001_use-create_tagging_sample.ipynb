{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a5566c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (4.63.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (4.63.2)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m911.0/911.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 wasabi-1.1.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting spacy-language-detection\n",
      "  Downloading spacy_language_detection-0.2.1-py3-none-any.whl (6.5 kB)\n",
      "Collecting langdetect==1.0.9\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy-language-detection) (3.5.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from langdetect==1.0.9->spacy-language-detection) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (4.63.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.21.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (8.1.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->spacy>=3.0.0->spacy-language-detection) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0.0->spacy-language-detection) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->spacy-language-detection) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->spacy-language-detection) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->spacy-language-detection) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->spacy>=3.0.0->spacy-language-detection) (2.1.1)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=084282a514e767e2d6025b567a8ab327db743f4f0d63fd2b86c76a748bcc2c96\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c4/16/af/1889804d8b7c0c041cadee8e29673a938a332acbf2865c70a1\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, spacy-language-detection\n",
      "Successfully installed langdetect-1.0.9 spacy-language-detection-0.2.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (65.6.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (1.21.6)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=286957 sha256=f6d50b1aa511ae51bb6ed6e4f55e8217e4e0d417986361f4064459760b56b55b\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/22/93/1a3d535655339964fd8936d807ec85da466303d545023d2139\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting p_tqdm\n",
      "  Downloading p_tqdm-1.4.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.45.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (4.63.2)\n",
      "Requirement already satisfied: pathos>=0.2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (0.3.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (1.16.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.3.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: p_tqdm\n",
      "  Building wheel for p_tqdm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for p_tqdm: filename=p_tqdm-1.4.0-py3-none-any.whl size=5383 sha256=5c63ff43440249bf43e7fd4df657fe259cb2c51ed1d948b6e91fe5208c8d12f3\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a2/54/28/97311859e49a55634542c9c839e4341fa00117ef1ae7ad0ffe\n",
      "Successfully built p_tqdm\n",
      "Installing collected packages: p_tqdm\n",
      "Successfully installed p_tqdm-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install nltk \n",
    "!pip install spacy\n",
    "!pip install spacy-language-detection\n",
    "!pip install fasttext\n",
    "!pip install langdetect\n",
    "!pip install p_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4daa691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import spacy\n",
    "from spacy_language_detection import LanguageDetector\n",
    "from spacy.language import Language\n",
    "import fasttext\n",
    "from langdetect import detect\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from p_tqdm import p_imap\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f60a56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.sent_end_chars=(':','.',',',';')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) - set(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'elven', 'twelve' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cleaning - utility functions: text_cleaning.py\n",
    "def basic_cleaning(text:string, punctuation:bool) -> str:\n",
    "    \"\"\"\n",
    "    This function does some basic data cleaning.\n",
    "    \"\"\"\n",
    "    for a_sign in ['\\\\n', '\\\\t', '☐', '☒', '\\xa0', '●', '“', '”']:\n",
    "        text = text.replace(a_sign,\" \")\n",
    "\n",
    "    if punctuation:\n",
    "        for a_punc in string.punctuation:\n",
    "            text = text.replace(a_punc, \" \")\n",
    "    else:\n",
    "        text =  re.sub(', ', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub('\\s+',\" \", text).lower()\n",
    "    return text.strip() \n",
    "\n",
    "\n",
    "def split_into_sentences(text:str) -> list:\n",
    "    #.\n",
    "    text = re.sub(r'([a-zA-Z)])\\.([a-zA-Z-])', r'\\1. \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\.([a-zA-Z-])\", r\"\\1. \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\.(\\d)\", r\"\\1. \\2\", text)\n",
    "    #.\n",
    "    text = re.sub(r'([a-zA-Z)])\\!([a-zA-Z-])', r'\\1! \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\!([a-zA-Z-])\", r\"\\1! \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\!(\\d)\", r\"\\1! \\2\", text)\n",
    "    #?\n",
    "    text = re.sub(r'([a-zA-Z)])\\?([a-zA-Z-])', r'\\1? \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\?([a-zA-Z-])\", r\"\\1? \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\?(\\d)\", r\"\\1? \\2\", text)\n",
    "    #:\n",
    "    text = re.sub(r'([a-zA-Z)])\\:([a-zA-Z-])', r'\\1: \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\:([a-zA-Z-])\", r\"\\1: \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\:(\\d)\", r\"\\1: \\2\", text)\n",
    "    #;\n",
    "    text = re.sub(r'([a-zA-Z)])\\;([a-zA-Z-])', r'\\1; \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\;([a-zA-Z-])\", r\"\\1; \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\;(\\d)\", r\"\\1; \\2\", text)\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_special_characters(text:str):\n",
    "    text = re.sub(r\"%\", ' percent ', text)\n",
    "    return re.sub(r\"[^A-Za-z0-9\\s!.,?]+\", '', text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text:string):\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    return text \n",
    "\n",
    "def clean_contractions(text:string):\n",
    "    text = contractions.fix(text).lower()\n",
    "    text = re.sub('can t', ' cannot ', text).strip()\n",
    "    text = re.sub(' ll ', ' will ', text).strip()\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    text = re.sub(r' [^\\w\\s] ', '', text)\n",
    "    return text\n",
    "\n",
    "def removing_numbers(text:string):\n",
    "    text = re.sub('(?<=\\d),(?=\\s)', '', text)\n",
    "    text = re.sub('(?<=\\d).(?=\\d)', '', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    #text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def removing_non_english(text:string):\n",
    "    text = ' '.join([word for word in text.split() if word in words.words()])\n",
    "    return text\n",
    "\n",
    "def lammatize_text(text:string):\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() ])\n",
    "    return text\n",
    "\n",
    "def stem_text(text:string):\n",
    "    text = ' '.join([ stemmer.stem(word) for word in text.split() ])\n",
    "    return text\n",
    "\n",
    "def replace_words(text, word_list: dict):\n",
    "    for word in list(word_list):\n",
    "        text = text.replace(word, word_list[word])\n",
    "    return text\n",
    "\n",
    "def text_cleaner(text:string, punctuation:bool, special_char: bool, stop_words:bool, contractions:bool, numbers:bool, non_english:bool, lammatize:bool, stem:bool, replacements:dict ):\n",
    "    text = basic_cleaning(text, punctuation)\n",
    "    if replacements != None:\n",
    "        text = replace_words(text, replacements)\n",
    "    if special_char:\n",
    "        text = remove_special_characters(text)\n",
    "    if contractions:\n",
    "        text = clean_contractions(text)\n",
    "    if numbers:\n",
    "        text = removing_numbers(text)\n",
    "    if non_english:\n",
    "        text = removing_non_english(text)\n",
    "    if lammatize:\n",
    "        text = lammatize_text(text)\n",
    "    if stem:\n",
    "        text = stem_text(text)\n",
    "    if stop_words:\n",
    "        text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#### Creating N-grams\n",
    "\n",
    "def creating_bigrams(text:str):\n",
    "    ''' This function returns bigrams. '''\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    bigrams = []\n",
    "    for sentence in sentences:\n",
    "        bigrams.extend(list(nltk.bigrams(sentence.split())))\n",
    "    return bigrams\n",
    "\n",
    "def creating_unigrams(text:str):\n",
    "    ''' This function returns unigrams. '''\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "#### Detect Language\n",
    "\n",
    "class language_detection_spacy():\n",
    "    def __init__(self):\n",
    "        def get_lang_detector(nlp, name):\n",
    "                return LanguageDetector(seed=42)  # We use the seed 42\n",
    "\n",
    "        self.nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "        Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "        self.nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "    def detect_language(self, text:str):\n",
    "        ''' This function detects the language of the text. '''\n",
    "        doc = self.nlp_model(text)\n",
    "        language = doc._.language\n",
    "        return language['language'], language['score']\n",
    "\n",
    "\n",
    "class languate_detection_fasttext():\n",
    "\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = os.path.join(os.getcwd(),\"lid.176.bin\")\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def detect_language(self, text):\n",
    "        predictions = self.model.predict(text, k=1) # returns top 2 matching languages\n",
    "        return predictions[0][0].replace('__label__', ''), predictions[1][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88e23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "language_detection = languate_detection_fasttext()\n",
    "# language_detection.detect_language('hello my name is kennedy')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8be8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2354609/2354609 [01:24<00:00, 27817.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# convert this to a function once check is complete: \n",
    "amazon_reviews_0_df = pd.read_json('/home/ec2-user/SageMaker/tag_data_creation/raw_dataframes/amazon_reviews_1_all_prod.json') # 1000 products \n",
    "amazon_reviews_0_df.review_body = amazon_reviews_0_df.review_body.apply(lambda x : x.replace(\"\\n\", \" \").strip())\n",
    "amazon_reviews_0_df.review_body = amazon_reviews_0_df.review_body.replace(\"\\s+\", \" \")\n",
    "amazon_reviews_0_df.rating = amazon_reviews_0_df.rating.apply(lambda x: float(x.replace(\" out of 5 stars\", \"\")))\n",
    "amazon_reviews_0_df['language']= amazon_reviews_0_df.review_body.progress_apply(lambda x: language_detection.detect_language(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437af34d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_reviews_rating_references(amazon_reviews:DataFrame, max_reviews = None) -> DataFrame:\n",
    "    '''\n",
    "    This function takes a DataFrame containing reviews, splits the review in sentences, looks for references to rating or reviews, and returns a new DataFrame.\n",
    "    If desired, maximum number of reviews can be set. \n",
    "    '''\n",
    "    amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n",
    "    if max_reviews:\n",
    "        print(f\"Process only {max_reviews} of {len(amazon_reviews)}\")\n",
    "        amazon_reviews = amazon_reviews[0:max_reviews]\n",
    "    else:\n",
    "        print(f\"Process all {len(amazon_reviews)}.\")\n",
    "\n",
    "    def processing_reviews(chunk:list):\n",
    "        amazon_review_cleaned = []\n",
    "        for review in chunk:\n",
    "            senteces = split_into_sentences(text = review['review_body'])\n",
    "            for sentence in senteces:\n",
    "                temp = {\"asin\": review[\"product_id\"], 'review_id': review[\"review_id\"], 'reviewer_id': review[\"account_link\"], 'date': review[\"review_date\"], \"full_review\": review[\"review_body\"] }\n",
    "                temp['sentence'] =  sentence\n",
    "                temp['text_clean'] = text_cleaner(text = sentence, non_english= False,  punctuation= True, special_char= True, stop_words= True, contractions= True, numbers= False, lammatize= True, stem = True, replacements= None)\n",
    "                temp['text_unigrams'] = creating_unigrams(temp['text_clean'])\n",
    "                temp['rating_reference'] =  1 if (\"rate\" in temp['text_unigrams']) else 0\n",
    "                temp['star_reference'] =  1 if (\"star\" in temp['text_unigrams']) else 0\n",
    "                temp['review_reference'] =  1 if (\"review\" in temp['text_unigrams']) else 0\n",
    "                amazon_review_cleaned.append(temp)\n",
    "                del temp\n",
    "        return amazon_review_cleaned\n",
    "\n",
    "    print(\"Start multi-processing!\")\n",
    "    chunk_size = 100\n",
    "    chunks = [amazon_reviews[i:i + chunk_size] for i in range(0, len(amazon_reviews), chunk_size)]\n",
    "    print(len(chunks))\n",
    "    multi_processing_output = [ x for x in p_imap(processing_reviews, chunks )]\n",
    "    print(len(multi_processing_output))\n",
    "\n",
    "    return DataFrame([item for sublist in multi_processing_output  for item in sublist]), multi_processing_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300e9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13444/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 241693.\n",
      "Start multi-processing!\n",
      "2417\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.05867791175842285,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2417,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b951610bb6fc42f7986ecbd0302fdf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417\n"
     ]
    }
   ],
   "source": [
    "amazon_reviews = amazon_reviews_0_df.copy()\n",
    "amazon_reviews= amazon_reviews[['review_id', 'account_link', 'review_date', 'rating',\n",
    "       'review_title', 'review_body', 'purchase_type', 'helpful_votes',\n",
    "       'product_id', 'overall_rating', 'purchase_type', 'review_body',\n",
    "       'language']]\n",
    "amazon_reviews_df, multi_processing_output = clean_reviews_rating_references(amazon_reviews=amazon_reviews[amazon_reviews['rating'] <= 2 ])\n",
    "amazon_reviews_df_temp = amazon_reviews_df[(amazon_reviews_df['rating_reference'] == 1) | (amazon_reviews_df['star_reference'] == 1) | (amazon_reviews_df['review_reference'] == 1)].reset_index(drop = True) \n",
    "amazon_reviews_df_temp[\"sentence\"] = amazon_reviews_df_temp[\"sentence\"].apply(lambda x: ' ' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc583e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_df_temp.to_excel('/home/ec2-user/SageMaker/tag_data_creation/manual_tagging_output/amazon_reviews_1_all_prod.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a64221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33329"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amazon_reviews_df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db52f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
